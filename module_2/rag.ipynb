{
 "cells": [
  {
   "cell_type": "code",
   "id": "ae8e2ae0-da25-44e7-ae82-024173150a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:49.113195Z",
     "start_time": "2025-06-28T12:53:48.386200Z"
    }
   },
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "57de60e5-b96c-499c-a7cf-0f30fc33b324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:49.214955Z",
     "start_time": "2025-06-28T12:53:49.211235Z"
    }
   },
   "source": [
    "documents[2]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - Can I still join the course after the start date?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "ce7d0d18-5c07-4010-9f90-bbd021f110c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.368772Z",
     "start_time": "2025-06-28T12:53:49.290388Z"
    }
   },
   "source": [
    "import minsearch\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x10392fe80>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "aa755a08-b98d-4e92-8994-04e6108499d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.720926Z",
     "start_time": "2025-06-28T12:53:52.382519Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "b21237c3-80e9-429c-a089-d45428087046",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.733461Z",
     "start_time": "2025-06-28T12:53:52.730898Z"
    }
   },
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8cc5784e-6515-42e5-be62-8fb915df1088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.745571Z",
     "start_time": "2025-06-28T12:53:52.742256Z"
    }
   },
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT: \n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "97d35dec-c25f-472d-b961-20d5c30902ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.754867Z",
     "start_time": "2025-06-28T12:53:52.752350Z"
    }
   },
   "source": [
    "def llm(prompt):\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "8602f40b-ad3b-49c9-b3cc-051a79c888bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:52.770244Z",
     "start_time": "2025-06-28T12:53:52.768012Z"
    }
   },
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "5fd4497b-c5d5-4258-b950-6b35d1af4ec5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:53:56.700075Z",
     "start_time": "2025-06-28T12:53:52.782830Z"
    }
   },
   "source": [
    "rag('how do I run kafka?')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka, you need to use the terminal commands appropriate for either Java or Python environments:\\n\\n1. **For Java Kafka**: In your project directory, you can run the producer or consumer by executing:\\n   ```\\n   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n   ```\\n\\n2. **For Python Kafka**: If you encounter issues like \"ModuleNotFoundError\", make sure to use \\n   ```\\n   pip install kafka-python-ng\\n   ```\\n   and if you\\'re having trouble with the module when using a script, ensure you have created and activated a virtual environment, and run the dependencies listed in `requirements.txt`.\\n\\nMake sure all necessary services and dependencies are set up before trying to run your Kafka applications.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "385b012f-4905-422d-8d7c-3d542dfe5a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:54:00.889263Z",
     "start_time": "2025-06-28T12:53:56.717490Z"
    }
   },
   "source": [
    "rag('the course has already started, can I still enroll?')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still enroll in the course after it has started. Even if you don't register, you're still eligible to submit the homework assignments. However, keep in mind that there are deadlines for turning in the final projects, so it's important not to leave everything until the last minute.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "71154c38-cc68-4d29-9809-f0f0545c79c3",
   "metadata": {},
   "source": [
    "## RAG with Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "id": "a78f5cb1-709c-4b48-a9b9-1738b75415de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:54:02.657758Z",
     "start_time": "2025-06-28T12:54:00.908105Z"
    }
   },
   "source": [
    "from qdrant_client import QdrantClient, models"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "61784d3a-b8f1-45b2-9404-e3495dca1152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:54:02.695286Z",
     "start_time": "2025-06-28T12:54:02.673636Z"
    }
   },
   "source": [
    "qd_client = QdrantClient(\"http://localhost:6333\")"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "f435b8da-834e-45c1-b83e-e28c294c044d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:54:02.703616Z",
     "start_time": "2025-06-28T12:54:02.701986Z"
    }
   },
   "source": [
    "EMBEDDING_DIMENSIONALITY = 512\n",
    "model_handle = \"jinaai/jina-embeddings-v2-small-en\""
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "b4fa85d4-af1d-46b9-a281-87c5f332bef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:54:02.713465Z",
     "start_time": "2025-06-28T12:54:02.711282Z"
    }
   },
   "source": [
    "collection_name = \"zoomcamp-faq\""
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "fce5957c-22c2-43e6-8dc1-af0c1acc884e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:55:19.463120Z",
     "start_time": "2025-06-28T12:55:19.407152Z"
    }
   },
   "source": [
    "qd_client.delete_collection(collection_name=collection_name)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "722e75a1-95ab-4388-94c0-800ec4f58866",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:55:21.004778Z",
     "start_time": "2025-06-28T12:55:20.830084Z"
    }
   },
   "source": [
    "qd_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=EMBEDDING_DIMENSIONALITY,\n",
    "        distance=models.Distance.COSINE\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "cb29d2a0-64df-4ea2-8920-8e6a16c4bcd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:55:22.042741Z",
     "start_time": "2025-06-28T12:55:21.957335Z"
    }
   },
   "source": [
    "qd_client.create_payload_index(\n",
    "    collection_name=collection_name,\n",
    "    field_name=\"course\",\n",
    "    field_schema=\"keyword\"\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "1348ef23-364a-4719-94fe-7a9cf8ea8371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:55:23.387426Z",
     "start_time": "2025-06-28T12:55:23.371032Z"
    }
   },
   "source": [
    "points = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    text = doc['question'] + ' ' + doc['text']\n",
    "    vector = models.Document(text=text, model=model_handle)\n",
    "    point = models.PointStruct(\n",
    "        id=i,\n",
    "        vector=vector,\n",
    "        payload=doc\n",
    "    )\n",
    "    points.append(point)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "e5e0e480-2af4-4a0e-b3d0-28a1e2181195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T12:55:30.684817Z",
     "start_time": "2025-06-28T12:55:24.726342Z"
    }
   },
   "source": [
    "qd_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55b55b77a5f24e5a910564d9b2e0d721"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "onnx/model.onnx:   0%|          | 0.00/130M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c2309b9ff9784744ab41268ad423464d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/367 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "056b6a254aa048d3b5a3a081c0e6a007"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7ec1e3f28444eee83e1a13cbb873624"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d35c4217ee24a348b6d133922617c31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb38b01830a342e1815543f5a536be54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mqd_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcollection_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollection_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpoints\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpoints\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/qdrant_client.py:1627\u001B[0m, in \u001B[0;36mQdrantClient.upsert\u001B[0;34m(self, collection_name, points, wait, ordering, shard_key_selector, **kwargs)\u001B[0m\n\u001B[1;32m   1619\u001B[0m         points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\n\u001B[1;32m   1620\u001B[0m             \u001B[38;5;28miter\u001B[39m(\n\u001B[1;32m   1621\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_models(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1624\u001B[0m             )\n\u001B[1;32m   1625\u001B[0m         )\n\u001B[1;32m   1626\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1627\u001B[0m         points \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1628\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embed_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1629\u001B[0m \u001B[43m                \u001B[49m\u001B[43mpoints\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlocal_inference_batch_size\u001B[49m\n\u001B[1;32m   1630\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1631\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1633\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mupsert(\n\u001B[1;32m   1634\u001B[0m     collection_name\u001B[38;5;241m=\u001B[39mcollection_name,\n\u001B[1;32m   1635\u001B[0m     points\u001B[38;5;241m=\u001B[39mpoints,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1639\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   1640\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/qdrant_fastembed.py:875\u001B[0m, in \u001B[0;36mQdrantFastembedMixin._embed_models\u001B[0;34m(self, raw_models, is_query, batch_size)\u001B[0m\n\u001B[1;32m    867\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_embed_models\u001B[39m(\n\u001B[1;32m    868\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    869\u001B[0m     raw_models: Union[BaseModel, Iterable[BaseModel]],\n\u001B[1;32m    870\u001B[0m     is_query: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    871\u001B[0m     batch_size: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    872\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterable[BaseModel]:\n\u001B[1;32m    873\u001B[0m     FastEmbedMisc\u001B[38;5;241m.\u001B[39mimport_fastembed()\n\u001B[0;32m--> 875\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_model_embedder\u001B[38;5;241m.\u001B[39membed_models(\n\u001B[1;32m    876\u001B[0m         raw_models\u001B[38;5;241m=\u001B[39mraw_models,\n\u001B[1;32m    877\u001B[0m         is_query\u001B[38;5;241m=\u001B[39mis_query,\n\u001B[1;32m    878\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mDEFAULT_BATCH_SIZE,\n\u001B[1;32m    879\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:71\u001B[0m, in \u001B[0;36mModelEmbedder.embed_models\u001B[0;34m(self, raw_models, is_query, batch_size)\u001B[0m\n\u001B[1;32m     69\u001B[0m     raw_models \u001B[38;5;241m=\u001B[39m [raw_models]\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m raw_models_batch \u001B[38;5;129;01min\u001B[39;00m iter_batch(raw_models, batch_size):\n\u001B[0;32m---> 71\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_models_batch(\n\u001B[1;32m     72\u001B[0m         raw_models_batch, is_query, inference_batch_size\u001B[38;5;241m=\u001B[39mbatch_size\n\u001B[1;32m     73\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:147\u001B[0m, in \u001B[0;36mModelEmbedder.embed_models_batch\u001B[0;34m(self, raw_models, is_query, inference_batch_size)\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m raw_models\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 147\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m (\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_model(\n\u001B[1;32m    149\u001B[0m             raw_model,\n\u001B[1;32m    150\u001B[0m             is_query\u001B[38;5;241m=\u001B[39mis_query,\n\u001B[1;32m    151\u001B[0m             accumulating\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    152\u001B[0m             inference_batch_size\u001B[38;5;241m=\u001B[39minference_batch_size,\n\u001B[1;32m    153\u001B[0m         )\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m raw_model \u001B[38;5;129;01min\u001B[39;00m raw_models\n\u001B[1;32m    155\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:148\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m raw_models\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    147\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m (\n\u001B[0;32m--> 148\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m            \u001B[49m\u001B[43mraw_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m            \u001B[49m\u001B[43mis_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m            \u001B[49m\u001B[43maccumulating\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m            \u001B[49m\u001B[43minference_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_batch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    153\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m raw_model \u001B[38;5;129;01min\u001B[39;00m raw_models\n\u001B[1;32m    155\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:232\u001B[0m, in \u001B[0;36mModelEmbedder._process_model\u001B[0;34m(self, model, paths, is_query, accumulating, inference_batch_size)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m accumulating:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    230\u001B[0m         inference_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    231\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minference_batch_size should be passed for inference\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 232\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    233\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_drain_accumulator(\n\u001B[1;32m    234\u001B[0m             data, is_query\u001B[38;5;241m=\u001B[39mis_query, inference_batch_size\u001B[38;5;241m=\u001B[39minference_batch_size\n\u001B[1;32m    235\u001B[0m         )\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m current_model\n\u001B[1;32m    237\u001B[0m     ]\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m was_list:\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(item, path\u001B[38;5;241m.\u001B[39mcurrent, embeddings)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:233\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m accumulating:\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[1;32m    230\u001B[0m         inference_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    231\u001B[0m     ), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minference_batch_size should be passed for inference\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    232\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m--> 233\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drain_accumulator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    234\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minference_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_batch_size\u001B[49m\n\u001B[1;32m    235\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m current_model\n\u001B[1;32m    237\u001B[0m     ]\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m was_list:\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28msetattr\u001B[39m(item, path\u001B[38;5;241m.\u001B[39mcurrent, embeddings)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:315\u001B[0m, in \u001B[0;36mModelEmbedder._drain_accumulator\u001B[0;34m(self, data, is_query, inference_batch_size)\u001B[0m\n\u001B[1;32m    312\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m data  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    314\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_storage \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_storage\u001B[38;5;241m.\u001B[39mget(data\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 315\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embed_accumulator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mis_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minference_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_batch_size\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_embed(data\u001B[38;5;241m.\u001B[39mmodel)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:392\u001B[0m, in \u001B[0;36mModelEmbedder._embed_accumulator\u001B[0;34m(self, is_query, inference_batch_size)\u001B[0m\n\u001B[1;32m    389\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is not among supported models\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    391\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_accumulator\u001B[38;5;241m.\u001B[39mitems():\n\u001B[0;32m--> 392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_storage[model] \u001B[38;5;241m=\u001B[39m \u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m        \u001B[49m\u001B[43mobjects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minference_batch_size\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_batch_accumulator\u001B[38;5;241m.\u001B[39mclear()\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/model_embedder.py:361\u001B[0m, in \u001B[0;36mModelEmbedder._embed_accumulator.<locals>.embed\u001B[0;34m(objects, model_name, batch_size)\u001B[0m\n\u001B[1;32m    356\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (options, is_text) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(unique_options, unique_options_is_text)):\n\u001B[1;32m    358\u001B[0m     embeddings\u001B[38;5;241m.\u001B[39mextend(\n\u001B[1;32m    359\u001B[0m         [\n\u001B[1;32m    360\u001B[0m             embedding\n\u001B[0;32m--> 361\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membedder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m                \u001B[49m\u001B[43mtexts\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m                \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatches\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mis_text\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m                \u001B[49m\u001B[43mis_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m                \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m                \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m         ]\n\u001B[1;32m    370\u001B[0m     )\n\u001B[1;32m    372\u001B[0m iter_embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(embeddings)\n\u001B[1;32m    373\u001B[0m ordered_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[NumericVector]] \u001B[38;5;241m=\u001B[39m [[]] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(objects)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/embedder.py:237\u001B[0m, in \u001B[0;36mEmbedder.embed\u001B[0;34m(self, model_name, texts, images, options, is_query, batch_size)\u001B[0m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m texts \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m FastEmbedMisc\u001B[38;5;241m.\u001B[39mis_supported_text_model(model_name):\n\u001B[0;32m--> 237\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_embed_dense_text\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m FastEmbedMisc\u001B[38;5;241m.\u001B[39mis_supported_sparse_model(model_name):\n\u001B[1;32m    241\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_sparse_text(\n\u001B[1;32m    242\u001B[0m             texts, model_name, options, is_query, batch_size\n\u001B[1;32m    243\u001B[0m         )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/embedder.py:280\u001B[0m, in \u001B[0;36mEmbedder._embed_dense_text\u001B[0;34m(self, texts, model_name, options, is_query, batch_size)\u001B[0m\n\u001B[1;32m    277\u001B[0m embedding_model_inst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_or_init_model(model_name\u001B[38;5;241m=\u001B[39mmodel_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions \u001B[38;5;129;01mor\u001B[39;00m {})\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_query:\n\u001B[0;32m--> 280\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    281\u001B[0m         embedding\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m embedding_model_inst\u001B[38;5;241m.\u001B[39membed(documents\u001B[38;5;241m=\u001B[39mtexts, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m    283\u001B[0m     ]\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    285\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    286\u001B[0m         embedding\u001B[38;5;241m.\u001B[39mtolist() \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m embedding_model_inst\u001B[38;5;241m.\u001B[39mquery_embed(query\u001B[38;5;241m=\u001B[39mtexts)\n\u001B[1;32m    287\u001B[0m     ]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/qdrant_client/embed/embedder.py:280\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    277\u001B[0m embedding_model_inst \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_or_init_model(model_name\u001B[38;5;241m=\u001B[39mmodel_name, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions \u001B[38;5;129;01mor\u001B[39;00m {})\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_query:\n\u001B[0;32m--> 280\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    281\u001B[0m         embedding\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m embedding_model_inst\u001B[38;5;241m.\u001B[39membed(documents\u001B[38;5;241m=\u001B[39mtexts, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m    283\u001B[0m     ]\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    285\u001B[0m     embeddings \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    286\u001B[0m         embedding\u001B[38;5;241m.\u001B[39mtolist() \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m embedding_model_inst\u001B[38;5;241m.\u001B[39mquery_embed(query\u001B[38;5;241m=\u001B[39mtexts)\n\u001B[1;32m    287\u001B[0m     ]\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/fastembed/text/text_embedding.py:187\u001B[0m, in \u001B[0;36mTextEmbedding.embed\u001B[0;34m(self, documents, batch_size, parallel, **kwargs)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed\u001B[39m(\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    167\u001B[0m     documents: Union[\u001B[38;5;28mstr\u001B[39m, Iterable[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    171\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterable[NumpyArray]:\n\u001B[1;32m    172\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03m    Encode a list of documents into list of embeddings.\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;124;03m        List of embeddings, one per document\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39membed(documents, batch_size, parallel, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/fastembed/text/onnx_embedding.py:283\u001B[0m, in \u001B[0;36mOnnxTextEmbedding.embed\u001B[0;34m(self, documents, batch_size, parallel, **kwargs)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21membed\u001B[39m(\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    263\u001B[0m     documents: Union[\u001B[38;5;28mstr\u001B[39m, Iterable[\u001B[38;5;28mstr\u001B[39m]],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    267\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterable[NumpyArray]:\n\u001B[1;32m    268\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;124;03m    Encode a list of documents into list of embeddings.\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;124;03m    We use mean pooling with attention so that the model can handle variable-length inputs.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;124;03m        List of embeddings, one per document\u001B[39;00m\n\u001B[1;32m    282\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 283\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_embed_documents(\n\u001B[1;32m    284\u001B[0m         model_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name,\n\u001B[1;32m    285\u001B[0m         cache_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_dir),\n\u001B[1;32m    286\u001B[0m         documents\u001B[38;5;241m=\u001B[39mdocuments,\n\u001B[1;32m    287\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[1;32m    288\u001B[0m         parallel\u001B[38;5;241m=\u001B[39mparallel,\n\u001B[1;32m    289\u001B[0m         providers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproviders,\n\u001B[1;32m    290\u001B[0m         cuda\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcuda,\n\u001B[1;32m    291\u001B[0m         device_ids\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice_ids,\n\u001B[1;32m    292\u001B[0m         local_files_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_local_files_only,\n\u001B[1;32m    293\u001B[0m         specific_model_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_specific_model_path,\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    295\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:130\u001B[0m, in \u001B[0;36mOnnxTextModel._embed_documents\u001B[0;34m(self, model_name, cache_dir, documents, batch_size, parallel, providers, cuda, device_ids, local_files_only, specific_model_path, **kwargs)\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_onnx_model()\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m iter_batch(documents, batch_size):\n\u001B[1;32m    129\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post_process_onnx_output(\n\u001B[0;32m--> 130\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monnx_embed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[1;32m    131\u001B[0m         )\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m parallel \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/fastembed/text/onnx_text_model.py:94\u001B[0m, in \u001B[0;36mOnnxTextModel.onnx_embed\u001B[0;34m(self, documents, **kwargs)\u001B[0m\n\u001B[1;32m     89\u001B[0m     onnx_input[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(\n\u001B[1;32m     90\u001B[0m         [np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(e), dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64) \u001B[38;5;28;01mfor\u001B[39;00m e \u001B[38;5;129;01min\u001B[39;00m input_ids], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64\n\u001B[1;32m     91\u001B[0m     )\n\u001B[1;32m     92\u001B[0m onnx_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preprocess_onnx_input(onnx_input, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 94\u001B[0m model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mONNX_OUTPUT_NAMES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monnx_input\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m OnnxOutputContext(\n\u001B[1;32m     96\u001B[0m     model_output\u001B[38;5;241m=\u001B[39mmodel_output[\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m     97\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39monnx_input\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, attention_mask),\n\u001B[1;32m     98\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39monnx_input\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, input_ids),\n\u001B[1;32m     99\u001B[0m )\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm_zoomcamp/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:273\u001B[0m, in \u001B[0;36mSession.run\u001B[0;34m(self, output_names, input_feed, run_options)\u001B[0m\n\u001B[1;32m    271\u001B[0m     output_names \u001B[38;5;241m=\u001B[39m [output\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_outputs_meta]\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_feed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_options\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m C\u001B[38;5;241m.\u001B[39mEPFail \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    275\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_fallback:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c9be7f7-2813-432b-8d73-3dff448dd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'I just discovered the course. Can I still join it?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6698670a-6fe7-4f51-83f7-281e80e06f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(question):\n",
    "    print('vector_search is used')\n",
    "    \n",
    "    course = 'data-engineering-zoomcamp'\n",
    "    query_points = qd_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=models.Document(\n",
    "            text=question,\n",
    "            model=model_handle \n",
    "        ),\n",
    "        query_filter=models.Filter( \n",
    "            must=[\n",
    "                models.FieldCondition(\n",
    "                    key=\"course\",\n",
    "                    match=models.MatchValue(value=course)\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        limit=5,\n",
    "        with_payload=True\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for point in query_points.points:\n",
    "        results.append(point.payload)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d80d18e3-c512-4f97-9f77-b1145fdb73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = vector_search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90bdc6f4-b6f8-4491-84f4-bbd8a6b9f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_search is used\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To run Kafka, you need to follow these steps based on your scripts:\\n\\n1. Make sure your Kafka broker is running. You can confirm this by running `docker ps`. If the broker is not active, navigate to the folder with your docker-compose yaml file and run `docker compose up -d` to start all instances.\\n\\n2. In your project directory, to run the producer, use the following command:\\n   ```\\n   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n   ```\\n\\n3. Ensure that the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in your Java scripts (e.g., JsonProducer.java, JsonConsumer.java) is set to the correct server URL. Also, verify that the cluster key and secrets in `src/main/java/org/example/Secrets.java` are updated with the correct values.\\n\\nBy following these steps, you should be able to run Kafka successfully.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do I run kafka?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886901ee-6e55-4295-a106-af25e6483da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
